# -*- coding: utf-8 -*-
"""BIG PROJECT PROGRESS PYTHON MINGGU KEENAM_KEL5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Ql5mKDpVvZvECAlVY0WHNV-WquX2Fm3

### Import The Libraries
"""

import numpy as np
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objs as go
import warnings
warnings.filterwarnings("ignore")

# For Pre-Processing
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder
from sklearn.preprocessing import PolynomialFeatures

# Splitting and Train For Dataset
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

# For Checking Testing Results
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn import linear_model, decomposition, datasets
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

# For Clustering
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from itertools import product
from yellowbrick.cluster import KElbowVisualizer
from yellowbrick.cluster import SilhouetteVisualizer
from scipy.spatial.distance import cdist
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.metrics import silhouette_score

# For Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
import math

# Using HyperTuning
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import IsolationForest
from sklearn.pipeline import Pipeline

# Using TensorFlow For ANN
from plotly.offline import iplot
import keras
from keras.utils import np_utils
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping
from keras.wrappers.scikit_learn import KerasClassifier
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Sequential
from keras import layers
from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, Activation

"""### Input The Dataset"""

df = pd.read_csv('student.csv')
df

"""### Description The Data"""

df.describe(include = 'all')

df.corr()

"""### Data Information"""

df.info()

df.count()

"""### Identify The Missing Value"""

missing = pd.DataFrame({
    'Data Kosong': df.isnull().sum(),
    'Data Duplikat': df.duplicated().sum(),
    'Data NaNN': df.isna().sum(),
    'Type Data': df.dtypes})
missing

"""### Data Visualization"""

df['interest'].value_counts().plot.bar()

sns.countplot(x = df['interest'], hue = 'will_go_to_college', data = df)

grouped = df.groupby(['will_go_to_college'])['average_grades'].mean()
grouped

go_college = ['False', 'True']
average_grades = [84.39134, 87.80306]
plt.bar(go_college, average_grades)
plt.show()

grouped = df.groupby(['will_go_to_college', 'gender'])['gender'].count()
grouped

import numpy as np
import matplotlib.pyplot as plt

# data to plot
male_college = [249, 266]
female_college = [251, 234]

# create plot
fig, ax = plt.subplots()
bar_width = 0.35
X = np.arange(2)

p1 = plt.bar(X, male_college, bar_width, color='b',
label='Male')

# The bar of second plot starts where the first bar ends
p2 = plt.bar(X + bar_width, female_college, bar_width,
color='r',
label='Female')

plt.xlabel('College Intentions')
plt.ylabel('Count Gender')
plt.title('Count Gender by Gender and College Intentions')
plt.xticks(X + (bar_width/2) , ("True", "False"))
plt.legend()

plt.tight_layout()
plt.show()

"""### Encoding Categorical Features"""

df['type_school'] = LabelEncoder().fit_transform(df['type_school'])
df['school_accreditation'] = LabelEncoder().fit_transform(df['school_accreditation'])
df['gender'] = LabelEncoder().fit_transform(df['gender'])
df['interest'] = LabelEncoder().fit_transform(df['interest'])
df['residence'] = LabelEncoder().fit_transform(df['residence'])
df['parent_was_in_college'] = LabelEncoder().fit_transform(df['parent_was_in_college'])
df['will_go_to_college'] = LabelEncoder().fit_transform(df['will_go_to_college'])
df.head(10)

"""### Feature Selection For Model"""

# Feature Selecting
X = df.drop(columns='will_go_to_college')
y = df.will_go_to_college

"""### Build A Model Classification Using Logistic Regression

Using 90-10 Splitting Train Dataset
"""

# Separating dependent and independent features
y = df['will_go_to_college']
X = df.drop(columns=['will_go_to_college'])

# Splitting the training and testing data (90% - 10%)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size = 0.1, random_state = 100)

# Transformasi/normalisasi data dengan MinMax
columns = ['parent_salary','house_area','parent_age','average_grades']
scaler = MinMaxScaler()
X_train_scale1 = scaler.fit_transform(X_train1[columns])
X_test_scale1 = scaler.transform(X_test1[columns])

X_train1[columns]=X_train_scale1
X_test1[columns]=X_test_scale1

# Model 1
model1 = LogisticRegression(multi_class="auto")
model1.fit(X_train1, y_train1)

# Prediction Model 1
y_pred1 = model1.predict(X_test1)
model1.score(X_test1, y_test1)

# Regression Model 1
print("coefficents :",model1.coef_ )
print("intercept :", model1.intercept_)

# Evaluate Parameter Model 1
print('accuracy_score: ',accuracy_score(y_test1, y_pred1))
print('f1_score: ',f1_score(y_test1, y_pred1, pos_label='positive', average='micro'))
print('recall_score: ',recall_score(y_test1, y_pred1, pos_label='positive', average='micro'))
print('precision_score: ',precision_score(y_test1, y_pred1, pos_label='positive', average='micro'))
print("mean squared error (MSE) : %.2f "% mean_squared_error(y_test1, y_pred1))
print("mean absolute error (MAE) : %.2f "% mean_absolute_error(y_test1, y_pred1))
print("roots mean squared error (RMSE) : %.2f "% math.sqrt(mean_squared_error(y_test1, y_pred1)))
print("r2_score : %.2f "% r2_score(y_test1, y_pred1))

# Confosuion Matrix Model 1
confusion_matrix1 = confusion_matrix(y_test1, y_pred1)
confusion_matrix1

print(classification_report(y_test1, y_pred1))

"""Using 80-20 Splitting Train Dataset"""

# Separating dependent and independent features
y = df['will_go_to_college']
X = df.drop(columns=['will_go_to_college'])

# Splitting the training and testing data (80% - 20%)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size = 0.2, random_state = 130)

# Transformasi/normalisasi data dengan MinMax
columns = ['parent_salary','house_area','parent_age','average_grades']
scaler = MinMaxScaler()
X_train_scale2 = scaler.fit_transform(X_train2[columns])
X_test_scale2 = scaler.transform(X_test2[columns])

X_train2[columns]=X_train_scale2
X_test2[columns]=X_test_scale2

# Model 2
model2 = LogisticRegression(multi_class="auto")
model2.fit(X_train2, y_train2)

# Prediction Model 2
y_pred2 = model2.predict(X_test2)
model2.score(X_test2, y_test2)

# Regression Model 2
print("coefficents :",model2.coef_ )
print("intercept :", model2.intercept_)

# Evaluate Parameter Model 2
print('accuracy_score: ',accuracy_score(y_test2, y_pred2))
print('f1_score: ',f1_score(y_test2, y_pred2, pos_label='positive', average='micro'))
print('recall_score: ',recall_score(y_test2, y_pred2, pos_label='positive', average='micro'))
print('precision_score: ',precision_score(y_test2, y_pred2, pos_label='positive', average='micro'))
print("mean squared error (MSE) : %.2f "% mean_squared_error(y_test2, y_pred2))
print("mean absolute error (MAE) : %.2f "% mean_absolute_error(y_test2, y_pred2))
print("roots mean squared error (RMSE) : %.2f "% math.sqrt(mean_squared_error(y_test2, y_pred2)))
print("r2_score : %.2f "% r2_score(y_test2, y_pred2))

# Confosuion Matrix Model 2
confusion_matrix2 = confusion_matrix(y_test2, y_pred2)
confusion_matrix2

print(classification_report(y_test2, y_pred2))

"""Using 70-30 Splitting Train Dataset"""

# Separating dependent and independent features
y = df['will_go_to_college']
X = df.drop(columns=['will_go_to_college'])

# Splitting the training and testing data (70% - 30%)
X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size = 0.3, random_state = 100)

# Transformasi/normalisasi data dengan MinMax
columns = ['parent_salary','house_area','parent_age','average_grades']
scaler = MinMaxScaler()
X_train_scale3 = scaler.fit_transform(X_train3[columns])
X_test_scale3 = scaler.transform(X_test3[columns])

X_train3[columns]=X_train_scale3
X_test3[columns]=X_test_scale3

# Model 3
model3 = LogisticRegression(multi_class="auto")
model3.fit(X_train3, y_train3)

# Prediction Model 3
y_pred3 = model3.predict(X_test3)
model3.score(X_test3, y_test3)

# Regression Model 3
print("coefficents :",model3.coef_ )
print("intercept :", model3.intercept_)

# Evaluate Parameter Model 3
print('accuracy_score: ',accuracy_score(y_test3, y_pred3))
print('f1_score: ',f1_score(y_test3, y_pred3, pos_label='positive', average='micro'))
print('recall_score: ',recall_score(y_test3, y_pred3, pos_label='positive', average='micro'))
print('precision_score: ',precision_score(y_test3, y_pred3, pos_label='positive', average='micro'))
print("mean squared error (MSE) : %.2f "% mean_squared_error(y_test3, y_pred3))
print("mean absolute error (MAE) : %.2f "% mean_absolute_error(y_test3, y_pred3))
print("roots mean squared error (RMSE) : %.2f "% math.sqrt(mean_squared_error(y_test3, y_pred3)))
print("r2_score : %.2f "% r2_score(y_test3, y_pred3))

# Confosion Matrix Model 3
confusion_matrix3 = confusion_matrix(y_test3, y_pred3)
confusion_matrix3

print(classification_report(y_test3, y_pred3))

"""### Logistic Regression With HyperTuning"""

# Create an scaler object
sc = StandardScaler()

# Create a pca object
pca = decomposition.PCA()

# Create a logistic regression object with an L2 penalty
logistic = LogisticRegression()

# create a pipeline of three steps, first standardize the data
# second, transform the data with PCA
# third, train a logistic regression on the data
pipe = Pipeline(steps = [('sc', sc),
                        ('pca', pca),
                        ('logistic', logistic)])
# create parameter space
# create a lis of a sequence of integers from 1 to 30 (the number of feature in x+1)
n_components = list(range(1, X.shape[1]+1,1))
# create a list of a values of the regularization penalty
C = np.logspace(-4, 4, 50)
# create a list of a values of the regularization penalty
penalty = ['l1', 'l2']
# create the dictionary of all the parameter option
parameters = dict(pca__n_components = n_components,
                 logistic__C = C,
                 logistic__penalty = penalty)

# conduct parameter optimization with pipeline
# create a grid search object
clf = GridSearchCV(pipe, parameters)

# fit the grid search
clf.fit(X, y)

# view the best parameter
print("Best Penalty :", clf.best_estimator_.get_params()['logistic__penalty'])
print("Best C :", clf.best_estimator_.get_params()['logistic__C'])
print("Best Number :", clf.best_estimator_.get_params()['pca__n_components'])

# Use the cross-validation to evaluate the model
CV_Result = cross_val_score(clf, X, y, cv=5, n_jobs=-1)
print(); print(CV_Result)
print(); print(CV_Result.mean())
print(); print(CV_Result.std())

"""### Classification Using ANN

Using 70-30 Splitting Train Dataset
"""

seed=123
np.random.seed(seed)

# Labelling
y_train11 = np_utils.to_categorical(y_train1, num_classes = 2)
y_test11 = np_utils.to_categorical(y_test1, num_classes = 2)

# Define The Model ANN
def get_model_ann(batch_size, learning_rate):
    model = Sequential()
    model.add(Dense(10,activation='relu', input_shape=(10,)))
    model.add(Dense(32,activation='relu'))
    model.add(Dense(64,activation='relu'))
    model.add(Dense(32,activation='relu'))
    model.add(Dense(2,activation='sigmoid')) 
    model.compile(loss ='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=learning_rate),metrics=['accuracy'])
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)
    history = model.fit(X_train1, y_train11, epochs=100, batch_size=batch_size, validation_data=(X_test1, y_test11), callbacks=[early_stopping])
    return model

# Mendefinisikan Model 
model_ann = KerasClassifier(build_fn=get_model_ann)

#Parameter yang dihypermarameter tuning
params_ann= {'batch_size': [4, 8],
             'learning_rate':[0.001, 0.0001]}

ann = GridSearchCV(model_ann, param_grid=params_ann)

history1 = ann.fit(X_train1, y_train11)

print("\n Parameter Terbaik  {}".format(history1.best_params_))
print("\n Accuracy Terbaik  {}".format(history1.best_score_))

# melihat hasil parameter terbaik
print("Best: %f using %s" % (history1.best_score_, history1.best_params_))
means = history1.cv_results_['mean_test_score']
stds = history1.cv_results_['std_test_score']
params = history1.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# mengambil model terbaik
best_model1 = history1.best_estimator_.model

# Define Again and input the learning rate
model = Sequential()
model.add(Dense(10,activation='relu', input_shape=(10,)))
model.add(Dense(32,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(2,activation='sigmoid')) # gunakan softmax: output > 1,
                                         # gunakan sigmoid: output = 1 
model.compile(loss ='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=0.001),metrics=['accuracy'])
model.summary()

early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)


history11 = model.fit(X_train1, y_train11, epochs=100, batch_size=4, validation_data=(X_test1, y_test11), callbacks=[early_stopping])

def plot_graphs(history11, string):
    plt.plot(history11.history[string], color = 'green')
    plt.plot(history11.history['val_'+string], color = 'orange')
    plt.xlabel('epoch')
    plt.xlabel(string)
    plt.legend([string, "val_"+string])
    plt.show()

plot_graphs(history11, 'accuracy')
plot_graphs(history11, 'loss')

evaluation = model.evaluate(X_test1, y_test11)

ann_pred1 = np.argmax(model.predict(X_test1),axis=1)
ann_pred1

y_test1 = np.argmax(y_test11,axis=1)
y_test1

print ('accuracy_score: ',accuracy_score(y_test1,ann_pred1))
print ('f1_score: ',f1_score(y_test1,ann_pred1, pos_label='positive', average='micro'))
print ('recall_score: ',recall_score(y_test1,ann_pred1, pos_label='positive', average='micro'))
print ('precision_score: ',precision_score(y_test1,ann_pred1, pos_label='positive', average='micro'))

"""### Clustering With K-Means"""

X_numerics = df[['parent_age', 'parent_salary', 'house_area', 'average_grades']]

model4 = KMeans(random_state=1)
visualizer = KElbowVisualizer(model4, k=(2,10))

visualizer.fit(X_numerics)
visualizer.show()
plt.show()

model4 = KMeans(random_state=1)
visualizer = KElbowVisualizer(model4, k=(2,10), metric='silhouette')

visualizer.fit(X_numerics)
visualizer.show()
plt.show()

"""Cluster = 2"""

KM_2_clusters = KMeans(n_clusters=2, init='k-means++').fit(X_numerics) # initialise and fit K-Means model

KM2_clustered = X_numerics.copy()
KM2_clustered.loc[:,'Cluster'] = KM_2_clusters.labels_

fig1, axes = plt.subplots(1, 2, figsize=(12, 5))

scat_1 = sns.scatterplot(x='parent_salary', y='average_grades', data=KM2_clustered,
                         hue='Cluster', ax=axes[0], palette='Set1', legend='full')

sns.scatterplot(x='parent_age', y='average_grades', data=KM2_clustered,
                hue='Cluster', palette='Set1', ax=axes[1], legend='full')

axes[0].scatter(KM_2_clusters.cluster_centers_[:, 1], KM_2_clusters.cluster_centers_[:, 2], marker='s', s=40, c="blue")
axes[1].scatter(KM_2_clusters.cluster_centers_[:, 0], KM_2_clusters.cluster_centers_[:, 2], marker='s', s=40, c="blue")

plt.show()

KM_clust_sizes = KM2_clustered.groupby('Cluster').size().to_frame()
KM_clust_sizes.columns = ["KM_size"]
KM_clust_sizes

model41 = KMeans(n_clusters=2, random_state=0)
visualizer = SilhouetteVisualizer(model41, colors='yellowbrick')
visualizer.fit(X_numerics)
visualizer.show()
plt.show()

"""Cluster = 5"""

KM_5_clusters = KMeans(n_clusters=5, init='k-means++').fit(X_numerics) # initialise and fit K-Means model

KM2_clustered = X_numerics.copy()
KM2_clustered.loc[:,'Cluster'] = KM_5_clusters.labels_

fig2, axes = plt.subplots(1, 2, figsize=(12, 5))

scat_2 = sns.scatterplot(x='parent_salary', y='average_grades', data=KM5_clustered,
                         hue='Cluster', ax=axes[0], palette='Set1', legend='full')

sns.scatterplot(x='parent_age', y='average_grades', data=KM5_clustered,
                hue='Cluster', palette='Set1', ax=axes[1], legend='full')

axes[0].scatter(KM_5_clusters.cluster_centers_[:, 1], KM_5_clusters.cluster_centers_[:, 2], marker='s', s=40, c="blue")
axes[1].scatter(KM_5_clusters.cluster_centers_[:, 0], KM_5_clusters.cluster_centers_[:, 2], marker='s', s=40, c="blue")

plt.show()

KM_clust_sizes = KM5_clustered.groupby('Cluster').size().to_frame()
KM_clust_sizes.columns = ["KM_size"]
KM_clust_sizes

model42 = KMeans(n_clusters=5, random_state=0)
visualizer = SilhouetteVisualizer(model42, colors='yellowbrick')
visualizer.fit(X_numerics)
visualizer.show()
plt.show()

features1= df.iloc[:, [5,6,7,8]].values
features1 = np.asarray(features1)
print(features1)

# Mententukan jumlah k cluster dengan Elbow Analysis
ScoreElbow = []
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k)
    label = kmeans.fit_predict(features1)
    
    ScoreElbow.append(kmeans.inertia_)
plt.figure(figsize=(10,5))
plt.plot(range(1, 10), ScoreElbow, 'r-')
plt.ylabel('Inertia')
plt.xlabel('n_clusters')

score = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k)
    label = kmeans.fit_predict(features1)
    
    siluet = silhouette_score(features1, label, metric='euclidean')
    score.append(siluet)
    
plt.figure(figsize=(10,5))
plt.plot(range(2, 11), score)
plt.ylabel('Silhouette')
plt.xlabel('n_clusters')

kmeans = KMeans(n_clusters=2,init='k-means++',
               max_iter=300,n_init=10, random_state=0)
pred_y = kmeans.fit_predict(features1)
df['cluster'] = pd.DataFrame(pred_y)
df

print('Silhouette Score:', silhouette_score(features1, pred_y))

"""### Clustering With DBScan """

eps_values = np.arange(8,12.75,0.25) # eps values to be investigated
min_samples = np.arange(3,10) # min_samples values to be investigated

DBSCAN_params = list(product(eps_values, min_samples))

no_of_clusters = []
sil_score = []

for p in DBSCAN_params:
    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(X_numerics)
    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))
    sil_score.append(silhouette_score(X_numerics, DBS_clustering.labels_))

from sklearn.metrics import silhouette_score

no_of_clusters = []
sil_score = []

for p in DBSCAN_params:
    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(X_numerics)
    unique_labels = np.unique(DBS_clustering.labels_)
    num_clusters = len(unique_labels)
    
    if num_clusters > 1:
        no_of_clusters.append(num_clusters)
        sil_score.append(silhouette_score(X_numerics, DBS_clustering.labels_))
    else:
        print("Only one cluster found. Adjust DBSCAN parameters or try a different algorithm.")



